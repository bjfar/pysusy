#Global MSSM likelihood calculator for PySUSY3
#Ben Farmer, 4 Jun 2012

#note: scipt causing strange library searches in loops (looking for 'random' module)
import numpy as np
from numpy import pi
import scipy.interpolate as interp
from scipy import exp, log, sqrt, log10
from scipy import absolute as abs
from scipy.special import erfinv
from scipy.stats import chi2
import common.likefuncs as LFs
import ConfigParser                                                     #standard module for parsing config files

#==========================================================
# Custom likelihood functions and function generators (larger than 1 liners)
#==========================================================
def squarklikefunc(smassIN,nmassIN):    #common likelihood function for remaining squarks
        smass = np.abs(smassIN)
        nmass = np.abs(nmassIN)
        return LFs.logsteplower(x=smass, limit=97 if smass-nmass>10 else 44)

#LHCb Bs->mu+mu- profile log-likelihood curve 
def LHCbBsmumulikefunc(LogLcurve,minX,maxX):    
   """
   Keyword Args:
   LogLcurve - function which returns the LHCb Delta LogL value for a given
      theory prediction.
   minX - minimum extent of LogL curve in Bsmumu ('x' axis)
   maxX - maximum extent of LogL curve in Bsmumu ('x' axis)
   """
   def Bsmumulikefunc(Bsmumu):
      """Computes the log-likelihood value for a given BR(Bs->mumu) value.
      Bsmumu - predicted value of branching ratio. This function is just a 
      wrapper around the interpolating function 'LogLcurve' to deal with 
      predictions outside the interpolating range, and with unit conversions.
      """
      # Curve defined in [10^-9] units, must convert theory value to match 
      Bsmumu=(10**9)*Bsmumu 
      # Deal with theory values outside the range covered by the LogL curve by
      # setting them to the values on the limits of the range
      if Bsmumu<minX: Bsmumu=minX
      if Bsmumu>maxX: Bsmumu=maxX
      DLogL = LogLcurve.__call__(Bsmumu) # Get logl value
      # The LHCb plot claims the y axis is Delta LogL, but this makes no sense
      # because it is upside down. I believe it is actually -2*LogL since this
      # seems to give the confidence intervals they claim, so we must convert
      # this to an ACTUAL log likelihood
      return float(-0.5*DLogL) #also don't want this returned as a 1-element numpy array
         
   #return likelihood function
   return Bsmumulikefunc
#==========================================================
# "Effective" priors
#==========================================================
#
# These "effective priors" are volume element factors which are NOT part of the
# transformation of the random numbers generated by the sampler, because they 
# rely on information computed by the programs in the run sequence. Instead,
# the volume element factor is multiplied into the likelihood, so the effective
# prior counts as just another piece of the likelihood function as far as pysusy # is concerned. Their effect on the scan is to change the prior actually used
# into a different "effective" one by folding in the appropriate volume factor.
#
# For more info on how this works see notebook #3 page 75-82, and the papers
# proposing the various effective priors
# implemented below (0705.0487, 0812.0536, 0907.0594, 1005.2525)
#
# NOTE: These effective priors involve Jacobians due to changing sets of 
# variables, so they are applicable only for the MODEL and PRIOR specified. If
# they are used with any other choice of model or prior they will generate errors,
# or at least rubbish results.
#
# These priors return an "effective" likehood contribution, so they must be 
# implemented through a dummy observable.

# DOYOUN'S NMSSM TUNING PRIOR GOES HERE

# log-Jacobian for the transformation T^l_{k,ms} (see Eq. 3.9 of Doyoun's notes)
def logJew(l,k,Al,Ak,s,t,mHu2,mHd2,MZ,g1,g2):
    """Effective prior factor for implementing naturalness prior for CNMSSM.
    Contributes a term to the likelihood.
    Note: This part computes the contribution due to switching m_s^2 and kappa
    for M_z^2 and tan(beta) at the EW scale.
    
    Args:
    The following parameters are all evaluated at low energy.
    l - lambda
    k - kappa
    Al - A_lambda
    Ak - A_kappa
    s - <S>, NMSSM singlet VEV 
    t - tan(beta)
    mHu2 - up type Higgs doublet mass
    mHd2 - down type Higgs doublet mass
    MZ - Z mass
    g1 - U(1) gauge coupling
    g2 - SU(2) gauge coupling
    """
    
    #define shorthand variables (see Doyoun's notes, equations 2.18, 2.21, 2.23)
    s2b = np.sin(2*np.arctan(t))   #sin(2*beta)
    gbar2 = g1**2 + g2**2          #sum of squares of U1 and SU2 gauge couplings 
    MZ2 = MZ**2                    #Z mass squared
    b1 = -l/s
    b2 = 1./(2*l*s*s) * 2*t/(t**2-1)**2 * (mHu2 - mHd2)
    b3 = -1./(4*l*s*s)
    e1 = (2*l*s*s2b - Al - 2*k*s)/s**2
    e2 = -(1-t**2)/(t*(1+t**2)) * (Al + k*s)/s
    e3 = -l*s2b/(gbar2 * s*s)
    f1 = -(4*k**2*s + k*Ak + 4*MZ2/(gbar2 * s*s) * t/(1+t**2) * l*Al)
    f2 = 4*MZ2/(gbar2*s*s) * (1-t**2)/(1+t**2)**2 * l*s*(Al + k*s)
    f3 = -(2*l**2/gbar2 - 4*l/(gbar2*s) * t/(1+t**2) * (Al + k*s))
    M = [[b1, e1, f1],
         [b2, e2, f2],
         [b3, e3, f3]]
    Jew = (1./b1) * np.linalg.det(M)     
    return np.log(np.abs(Jew)) #log-likelihood contribution

def logJrge(s,t,mHu2,mHd2,l ,k ,yt ,g1 ,g2 ,g3,
                          l0,k0,yt0,g10,g20,g30):
    """Effective prior factor for implementing naturalness prior for CNMSSM.
    Contributes a term to the likelihood.
    Note: This part computes the contribution due to 1-loop RGE running of
    m_s^2 and kappa from the low scale (SUSY~EW) to high scale (GUT). Don't
    need to bother with this if RGE running is only going to SUSY scale, since
    not very much change occurs (and this function assumes SUSY~EW, since we
    do not actually get EW values for parameters from NMSSMTools, only SUSY
    scalw values.
    
    **only valid for l,k up to 0.3. Perhaps compute for reference only - if
    factor gets large then we know 
    
    Args:
    s - <S>, NMSSM singlet VEV 
    t - tan(beta)
    mHu2 - up type Higgs doublet mass
    mHd2 - down type Higgs doublet mass
    l - lambda
    k - kappa
    yt - top yukawa coupling
    g1 - U(1) gauge coupling
    g2 - SU(2) gauge coupling
    g3 - SU(3) gauge coupling
    
    Note: suffix "0" indicates the GUT version of the parameter; EW/SUSY scale
    version is assumed otherwise.
    """
    
    #b1 and b2 defined identically as in Jew factor (see also Doyoun's notes, 
    # equations 2.18, 2.21, 2.23)
    b1 = -l/s
    b2 = 1./(2*l*s*s) * 2*t/(t**2-1)**2 * (mHu2 - mHd2)
    b3 = -1./(4*l*s*s)
    
    # Doyoun used a_i = g_i^2/4pi in his notation, but since 4pi's cancel anyway
    # in the ratios I have just directly used the g_i's (and squared them).
    rl = l/l0
    rk = k/k0
    ryt = yt/yt0
    ra1 = (g1/g10)**2
    ra2 = (g2/g20)**2
    ra3 = (g3/g30)**2
    
    print 'rl', rl
    print 'rk', rk
    print 'ryt', ryt
    print 'ra1', ra1
    print 'ra2', ra2
    print 'ra3', ra3
    print 'b1', b1
    print 'b2', b2
    print 'b3', b3
    
    """Trying to avoid overflows: work in log space instead
    try:
        A = rl**(4./3.) * \
            rk**(-4./9.) * \
            ryt**(-2./3.) * \
            ra1**(5./(27.*b1)) * \
            ra2**(1./b2) * \
            ra3**(-16./(9.*b3))
        Jrge = (k0**3/k) * A**6
        logJrge = np.log(np.abs(Jrge))
    except OverflowError:
        #if we get an overflow presumably a very small or very large number has
        #been encountered. If small, this gives a big
        #likelihood penalisation (unless it would miraculously be cancelled
        #elsewhere in the expression) so we assume this is a super fine-tuned
        #point and assign a big likelihood penalty.
        #if the number is very large I guess we are screwed because we can't
        #compute what it is. I don't think "natural" points should receive a
        #giant likelihood bonus like this though, so we will always assume this
        #error means the point is no good.
        logJrge = -999.
    """
    
    logA =  (4./3.) * np.log(rl) \
          - (4./9.) * np.log(rk) \
          - (2./3.) * np.log(ryt) \
          + (5./(27.*b1)) * np.log(ra1) \
          + (1./b2) * np.log(ra2) \
          - (16./(9.*b3)) * np.log(ra3)
    logJrge = 3*np.log(k0/k) + 6*logA
    # no abs should be necessary anywhere as the ratios should all be positive
    # numbers, I believe.  
    
    print 'logJrge', logJrge
    print ''
    return logJrge #log-likelihood contribution



#==========================================================
# BUILD LIKELIHOOD FUNCTION CALCULATOR CLASS
#==========================================================

class LikeFuncCalculator:
    """Computes the PySUSY likelihood dictionary and global likelihood
    Args:
    obsdict -- a dictionary containing a dictionaries of observables 
        from each program in the run sequence, e.g.
        obsdict = { 'spectrum':          {  'mH': 125,
                                            'mX': 1200,
                                         },
                    'darkmatter':        {  'omegah2': 0.1,
                                            'gmuon'  : 33e-10,
                                         }
                  }
    Attributes set:
    self.globlikefunc -- Global likelihood function. Used by CMSSM.py.
    """
    #==========================================================
    # BEGIN OBJECT INITIALISATION
    #==========================================================
    # Read in various data files and do pre-analysis required to
    # build the various likelihood functions
    
    def __init__(self,configfile,defconfig):
        """Gather information required to construct some of the more
        complicated likelihood functions"""
        #--------------------------------------------------------------
        # Read config file (path supplied by master config module)
        #--------------------------------------------------------------
        cfg = ConfigParser.RawConfigParser()
        # Get default config values
        # (Before reading the real config file we read in default settings, in
        # case any new ones are missing from older config files)
        print 'Reading default config file {0}'.format(defconfig)
        cfg.readfp(open(defconfig))
        #read config file
        print 'Reading config file {0}'.format(configfile)
        cfg.read(configfile)
        
        #need to know this for a few spectrum generator specific observables
        specgen = cfg.get('progoptions','spectrumgenerator') 
        #user must tell likelihood calculator where to find the data files
        #needed to build the likelihood functions.
        datadir = cfg.get('obsoptions','datadir') 

        #--------------------------------------------------------------
        # Load options (whether to exclude some observables from scan; 
        # likelihood values still computed and output just not used)
        #--------------------------------------------------------------
        
        #observables options
        #useHiggs            = cfg.getboolean('obsoptions', 'useHiggs')
        useLHCHiggs         = cfg.getboolean('obsoptions', 'useLHCHiggs')
        useBdecays          = cfg.getboolean('obsoptions', 'useBdecays')
        #useDMdirect         = cfg.getboolean('obsoptions', 'useDMdirect')
        centralreliclimit   = cfg.getboolean('obsoptions', 'centralreliclimit')
        #program options
        usemicrOmegas       = cfg.getboolean('progoptions','computerelic') \
                           or cfg.getboolean('progoptions','computedirdet') \
                           or cfg.getboolean('progoptions','computeindirdet')
        #useHDecay           = cfg.getboolean('progoptions', 'useHDecay')
        #likelihood options                                             #We do this so that the cfg file does not have to be read every loop by the likelihood function calculator
        #useMW               = cfg.getboolean('likefunc', 'useMW')
        #usedeltarho         = cfg.getboolean('likefunc', 'usedeltarho')
        useomegah2          = cfg.getboolean('likefunc', 'useomegah2')
        usedeltaamu         = cfg.getboolean('likefunc', 'usedeltaamu')
        #useDSL              = cfg.getboolean('likefunc', 'useDSL')
        #prior options
        prior = cfg.get('prioroptions', 'effprior')
        if prior==None:
            useJew = False
            useJrge = False
        if prior=='EW' or prior=='T1':  # set the appropriate prior factors to be used
            useJew = True
            useJrge = False
        if prior=='EW+RGE':
            useJew = True
            useJrge = True
        #if cfg.getboolean('prioroptions', 'uselog') and useCCRprior:
        #    raise ValueError('Attempted to activate CCR prior with log prior!\
#Must use flat prior instead! Please adjust options config file.')

        #===============================================================
        # ONCE OFF COMPUTATIONS
        #===============================================================
        # Construction of numerical likelihood functions from data files,
        # etc.
        
        #-----------------------------------------------------------------------
        # Read in LHCb loglikelihood curve for BR(Bs->mu+mu-) from digitized 
        # curve data
        #-----------------------------------------------------------------------
        #read in digitized curve (from suppl. material fig 11. 8 of 1211.2674 (LHCb))
        rawdata = np.loadtxt(datadir+'/LHCbBsmumuDlogl.dat', unpack=False, delimiter=" ")
        #(data in units of [10^-9])
    
        rawdata = map(tuple,rawdata)    #we don't want a numpy array here, just a list of tuples
        finaldata=np.array(rawdata)
       
        BsmumuLogLcurve = interp.interp1d(finaldata[:,0],finaldata[:,1],kind='linear')
        # find out what the range covered by the data is
        minBR = min(finaldata[:,0])
        maxBR = max(finaldata[:,0])
        
        #===============================================================
        
        #-----------------------------------------------------------------------
        # Build likelihood definitions
        #-----------------------------------------------------------------------
        
        #---relic density-----------------------------------------------
        if centralreliclimit==True:
            #(assumes neutralino relic density = dark matter relic density)
            omegalikefunc=LFs.lognormallike          
        else:
            #(assumes neutralino relic density < dark matter relic density)
            omegalikefunc=LFs.logerfupper  
        
        #--Bs -> mu+ mu- -----------------------------------------------
        #From LHCb data 
        Bsmumulikefunc = LHCbBsmumulikefunc(BsmumuLogLcurve,minBR,maxBR) 
        
        #=======================================================================
        # COMPUTE GLOBAL LIKELIHOOD FUNCTION 
        #=======================================================================
        def globlikefunc(obsdict):
            """Compute log likelihoods and global log likelihood
            Output format:
            likedict = {likename: (logL, uselike),...}
            """
            likedict = {}   #reset likelihood dictionary
            
            #Extract individual observables dictionaries from container dictionary
            specdict = obsdict['spectrum']
            decadict = obsdict['decay']
            if usemicrOmegas: darkdict = obsdict['darkmatter']
            
            #===========================================================
            # EFFECTIVE PRIOR
            #===========================================================
            #likedict['CCRprior'] = (CCRprior(specdict['BQ'],specdict['tanbQ'],specdict['muQ']), useCCRprior)
            
            #===============================================================
            #   DEFINE LIKELIHOOD CONTRIBUTIONS
            #===============================================================
            #-----------MAIN CONTRIBUTORS TO LIKELIHOOD-----------------
            
            # delta(a_mu)
            likedict['deltaamu'] = (LFs.lognormallike(x=specdict['deltaamu'], 
                mean=33.53e-10, sigma=8.24e-10), usedeltaamu)
                #1106.1315v1, Table 10, Solution B (most conservative)
            
            #=====Electroweak precision, RELIC DENSITY and (g-2)_mu=============
            if usemicrOmegas:
                #Dark matter relic density
                likedict['omegah2'] = (omegalikefunc(darkdict['omegah2'], \
                    0.1186, sqrt((0.0031)**2+(0.10*0.1186)**2)), useomegah2)         
                    # 2013 Planck data release (http://arxiv.org/abs/1303.5076)
                    # From table 2. Three numbers are relevant:
                    # Best fit, 68% confidence intercal
                    # 0.12029,     0.1196 +/- 0.0031  : Planck temp. data
                    # 0.11805,     0.1186 +/- 0.0031  :    + Planck lensing data
                    # 0.12038,     0.1199 +/- 0.0027  :    + WMAP low multipole
                    #                                 :      polarisation data
                    # Giving our somewhat arbitrary 10% extra theory uncertainty
                    # it doesn't matter which we choose (they are all within 2%
                    # of each other). Even the WMAP result (0.1123 +/- 0.0035) is
                    # only about 6% different from the central Planck result. So
                    # let's just use the "Planck only" data.
                    #
                    # old WMAP reference:
                    # 2010, 1001.4538v2, page 3 table 1 (WMAP + BAO + H0 mean)
                    # theory (second component) uncertainty stolen from 
                    # 1107.1715, need proper source.

            
            #=====Higgs constraints=============================================
            # We first need to find out which of the NMSSM Higgses is SM-like
            # Check the reduced couplings? Use those which are closest on 
            # average to 1? Crude, but good enough for first quick scan.
            
            Higgses = ['H1','H2','H3','A1','A2'] # Must match blockmap
            # Compute summed squared difference of couplings from 1
            rgsqdiff = {}
            for Higgs in Higgses:
                rgsqdiff[Higgs] = \
                    (specdict['rg_{0}_u'.format(Higgs)] - 1)**2 \
                  + (specdict['rg_{0}_d'.format(Higgs)] - 1)**2 \
                  + (specdict['rg_{0}_WZ'.format(Higgs)] - 1)**2 \
                  + (specdict['rg_{0}_g'.format(Higgs)] - 1)**2 \
                  + (specdict['rg_{0}_a'.format(Higgs)] - 1)**2 \
            # Get key of entry of rqsqdiff containing the min squared difference
            mostSMlike = min(rgsqdiff, key=rgsqdiff.get)
                
            likedict['mh_SMlike'] = (LFs.lognormallike(\
                x=specdict['M{0}'.format(mostSMlike)], \
                mean=126., sigma=sqrt(1.**2)), useLHCHiggs) 
                # Just a rough guess at the measured mass of the SM-like Higgs.
                # Replace this with a rigorous likelihood involving decay rates.
            
            #print mostSMlike, specdict['M{0}'.format(mostSMlike)], \
            #    rgsqdiff[mostSMlike]
                
            #===============Flavour constraints=========================
            # Branching ratios:
            # Switch on/off using "useBdecays" config option.
            # "Theory error" taken to be the larger of the upper/lower errors
            # returned by nmspec. ->UPDATE: now using LFs.logdoublenormal,
            # models upper and lower errors by seperate Gaussians.
            
            # BF(b->s gamma) (i.e. B_u/B_d -> X_s gamma)
            bsgexperr2 = (0.26e-4)**2+(0.09e-4)**2
            bsguperr2 = bsgexperr2+(specdict['bsgmo+eth']-specdict['bsgmo'])**2
            bsglwerr2 = bsgexperr2+(specdict['bsgmo-eth']-specdict['bsgmo'])**2              
            likedict['bsgmo'] = (LFs.logdoublenormal(x=specdict['bsgmo'],
                mean=3.55e-4, sigmaP=sqrt(bsguperr2), sigmaM=sqrt(bsglwerr2))
                                                                  , useBdecays)
            # HFAG, arXiv:1010.1589 [hep-ex], Table 129 (Average)
            # 0.09e-4 contribution added in accordance with newer HFAG edition:
            # HFAG, arXiv:1207.1158 [hep-ex], pg 203. (i.e. basically unchanged)

            # BR(B+ -> tau+ + nu_tau)
            btaunuexperr2 = (0.3e-4)**2
            btaunuuperr2 = btaunuexperr2+(specdict['B+taunu+eth']-specdict['B+taunu'])**2
            btaunulwerr2 = btaunuexperr2+(specdict['B+taunu-eth']-specdict['B+taunu'])**2              
            likedict['B+taunu'] = (LFs.logdoublenormal(x=specdict['B+taunu'],
                mean=1.67e-4, sigmaP=sqrt(btaunuuperr2), sigmaM=sqrt(btaunulwerr2))
                                                                  , useBdecays)
            # HFAG 1010.1589v3 (updated 6 Sep 2011), retrieved 12 Oct 2011
            # Table 127, pg 180
            # HFAG, arXiv:1207.1158 [hep-ex], pg 204. table 144
            # (basically unchanged from 2010 data, smaller uncertainity 
            # (0.39->0.3), mean unchanged.
            
            # BR(Bs -> mu+ mu-)
            # See comments where BsmumuLogLcurve is created from data files
            likedict['bmumu'] = (Bsmumulikefunc(specdict['bmumu']), useBdecays)
            #Folding theory error into this properly would be hard... need to
            #convolve it in for every point. Can't think of a better way.
            #Otherwise need to settle on something constant that can be
            #computed at the beginning of the run.
            #specdict['bmumu+eth']
            #specdict['bmumu-eth'] 
            
            
            
            #--------Effective prior factor-------------------------------------
            likedict['logJew'] = (\
                                logJew(specdict['lambda_Qsusy'],
                                       specdict['kappa_Qsusy'],
                                       specdict['Alambda_Qsusy'],
                                       specdict['Akappa_Qsusy'],
                                       specdict['mueff_Qsusy']/
                                           specdict['lambda_Qsusy'], # <S>=mueff/lambda
                                       specdict['TanB'],
                                       specdict['MHu^2_Qsusy'],
                                       specdict['MHd^2_Qsusy'],
                                       specdict['MZ'],
                                       specdict['g`_Qsusy'],
                                       specdict['g2_Qsusy'])                    
                    , False)
            likedict['logJrge'] = (\
                               logJrge(specdict['mueff_Qsusy']/
                                           specdict['lambda_Qsusy'], # <S>=mueff/lambda
                                       specdict['TanB'],
                                       specdict['MHu^2_Qsusy'],
                                       specdict['MHd^2_Qsusy'],
                                       specdict['lambda_Qsusy'],
                                       specdict['kappa_Qsusy'],
                                       specdict['yt_Qsusy'],
                                       specdict['g`_Qsusy'],
                                       specdict['g2_Qsusy'],
                                       specdict['g3_Qsusy'],
                                       specdict['lambda_QGUT'],
                                       specdict['kappa_QGUT'],
                                       specdict['yt_QGUT'],
                                       specdict['g`_QGUT'],
                                       specdict['g2_QGUT'],
                                       specdict['g3_QGUT'])                    
                    , False)
                    
            # Total combined effective log prior factor (for easy removal during
            # analysis)
            effprior = 0.
            if useJew:  effprior += likedict['logJew'][0]
            if useJrge: effprior += likedict['logJrge'][0]
            likedict['effprior'] = (effprior, True)
            
            #===============GLOBAL LOG LIKELIHOOD=======================
            LogL = sum( logl for logl,uselike in likedict.itervalues() if uselike )
            return LogL, likedict
            
        #===ASSIGN GLOBAL LIKELIHOOD FUNCTION TO ATTRIBUTE==============
        #------(so that it can be used by PySUSY)-----------------------
        self.globlikefunc = globlikefunc
            

        
        
